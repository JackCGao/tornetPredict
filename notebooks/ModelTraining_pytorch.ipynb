{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d1c7cf-0ed4-4dae-a5c1-884c98e0e927",
   "metadata": {},
   "source": [
    "# Training a simple CNN model using pytorch for Tornado Detection\n",
    "\n",
    "This notebook steps through how to train a simple CNN model using a subset of the dataset.\n",
    "\n",
    "This will not produce a model with any skill, but simply provides a working end-to-end example of how to set up a data loader, build, and fit a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce220886-991c-4e31-b3b1-de8e498271e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Uncomment if tornet isn't installed in your environment or in your path already\n",
    "#sys.path.append('../')  \n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from tornet.data.loader import read_file, TornadoDataLoader\n",
    "from tornet.data.preprocess import add_coordinates, remove_time_dim, permute_dims\n",
    "from tornet.data.constants import ALL_VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2961faa-31c7-4c16-9a54-2f1a1fe75997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basic dataloader\n",
    "# This option loads directly from netcdf files, and will be slow\n",
    "# To speed up training,\n",
    "#     rebuild dataset as array_record  (see tornet/data/tfds/tornet/README.md)\n",
    "\n",
    "data_root='/Users/jackgao/Downloads/data'\n",
    "\n",
    "data_type='train'\n",
    "years = [2013, 2014]\n",
    "\n",
    "catalog_path = os.path.join(data_root,'catalog.csv')\n",
    "if not os.path.exists(catalog_path):\n",
    "    raise RuntimeError('Unable to find catalog.csv at '+data_root)\n",
    "        \n",
    "catalog = pd.read_csv(catalog_path,parse_dates=['start_time','end_time'])\n",
    "catalog = catalog[catalog['type']==data_type]\n",
    "catalog = catalog[catalog.start_time.dt.year.isin(years)]\n",
    "catalog = catalog.sample(frac=1,random_state=1234) # shuffles list\n",
    "file_list = [os.path.join(data_root,f) for f in catalog.filename]\n",
    "\n",
    "# Dataset, with preprocessing\n",
    "class TornadoDataset(TornadoDataLoader,Dataset):\n",
    "    pass\n",
    "transform = transforms.Compose([\n",
    "            # add coordinates tensor to data\n",
    "            lambda d: add_coordinates(d,include_az=False,tilt_last=False,backend=torch), \n",
    "            # Remove time dimension\n",
    "            lambda d: remove_time_dim(d)])                                \n",
    "torch_ds = TornadoDataset(file_list,\n",
    "                          variables=ALL_VARIABLES,\n",
    "                          n_frames=1,\n",
    "                          tilt_last=False, # so ordering of dims is [time,tilt,az,range]\n",
    "                          transform=transform) \n",
    "                          \n",
    "# Torch data loader\n",
    "batch_size=32\n",
    "torch_dl = torch.utils.data.DataLoader( torch_ds, \n",
    "                                        batch_size=batch_size, \n",
    "                                        num_workers=8 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80bd951b-6faf-4294-9308-ff59c496261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If data was registered in tensorflow_dataset, run this cell instead\n",
    "# env variable TFDS_DATA_DIR should point to location of this resaved dataset\n",
    "\n",
    "#import tensorflow_datasets as tfds # need version >= 4.9.3\n",
    "#import tornet.data.tfds.tornet.tornet_dataset_builder # registers 'tornet'\n",
    "#from tornet.data.torch.loader import TFDSTornadoDataset\n",
    "#data_type='train'\n",
    "#years = [2017,]\n",
    "#ds = tfds.data_source('tornet')\n",
    "## Dataset, with preprocessing\n",
    "#transform = transforms.Compose([\n",
    "#            # transpose to [time,tile,az,rng]\n",
    "#            lambda d: permute_dims(d,(0,3,1,2)),\n",
    "#            # add coordinates tensor to data\n",
    "#            lambda d: add_coordinates(d,include_az=False,tilt_last=False,backend=torch), \n",
    "#            # Remove time dimension\n",
    "#            lambda d: remove_time_dim(d)])                                \n",
    "#datasets = [\n",
    "#     TFDSTornadoDataset(ds['%s-%d' % (data_type,y)] ,transform)  for y in years\n",
    "#     ]\n",
    "#dataset = torch.utils.data.ConcatDataset(datasets)\n",
    "#torch_dl = torch.utils.data.DataLoader( dataset, \n",
    "#                                      batch_size=32,\n",
    "#                                      num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ce7460f-b197-4a4c-a726-b7833f6d72c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tornet.models.torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create simple CNN model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtornet\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcnn_baseline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NormalizeVariable\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtornet\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CHANNEL_MIN_MAX\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mTornadoLikelihood\u001b[39;00m(nn.Module):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tornet.models.torch'"
     ]
    }
   ],
   "source": [
    "# Create simple CNN model\n",
    "from tornet.models.torch.cnn_baseline import NormalizeVariable\n",
    "from tornet.data.constants import CHANNEL_MIN_MAX\n",
    "\n",
    "\n",
    "class TornadoLikelihood(nn.Module):\n",
    "    \"\"\"\n",
    "    Template for CNN that produces likelihood field\n",
    "    \"\"\"\n",
    "    def __init__(self,radar_variables=ALL_VARIABLES):\n",
    "        super(TornadoLikelihood, self).__init__()\n",
    "        self.radar_variables=radar_variables\n",
    "        \n",
    "        # Set up normalizers\n",
    "        self.input_norm_layers = {}\n",
    "        for v in radar_variables:\n",
    "            min_max = np.array(CHANNEL_MIN_MAX[v]) # [2,]\n",
    "            scale = 1/(min_max[1]-min_max[0])\n",
    "            offset = min_max[0]\n",
    "            self.input_norm_layers[v] = NormalizeVariable(scale,offset)\n",
    "            \n",
    "        # Processing layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=12, out_channels=32, kernel_size=(3,3),padding='same')\n",
    "        # add more..\n",
    "        self.conv_out = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=(3,3),padding='same')\n",
    "        \n",
    "    def _normalize_inputs(self,data):\n",
    "        normed_data = {}\n",
    "        for v in self.radar_variables:\n",
    "            normed_data[v] = self.input_norm_layers[v](data[v])\n",
    "        return normed_data\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Assumes x contains radar varialbes on [batch,tilt,az,rng]\n",
    "        \"\"\"\n",
    "        # extract radar inputs\n",
    "        x = {v:x[v] for v in self.radar_variables} # each [batch,tilt,Az,Rng]\n",
    "        # normalize\n",
    "        x = self._normalize_inputs(x) # each [batch,tilt,Az,Rng]\n",
    "        # concatenate along channel (tilt) dim\n",
    "        x = torch.cat([x[v] for v in self.radar_variables],axis=1) #  [batch,tilt*len(radar_variables)*2,Az,Rng]\n",
    "        # Remove nan's\n",
    "        x = torch.where(torch.isnan(x),-3,x)\n",
    "        \n",
    "        # process\n",
    "        x = self.conv1(x)\n",
    "        # add more..\n",
    "        x = self.conv_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc3e24-c690-49d1-9c2b-f3bb46bf511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train this model using torch lightning\n",
    "import lightning as L\n",
    "import torchmetrics\n",
    "from torchmetrics import MetricCollection\n",
    "from tornet.models.torch.cnn_baseline import TornadoClassifier\n",
    "\n",
    "# Metrics expected to be binary classification metrics that expect (logits,label)\n",
    "#    where logits and label are both (N,) tensors \n",
    "#    e.g. torchmetrics.classification.BinaryAccuracy\n",
    "metrics = MetricCollection([\n",
    "            torchmetrics.classification.BinaryAccuracy(), \n",
    "            torchmetrics.classification.BinaryAUROC(), \n",
    "            torchmetrics.classification.BinaryAveragePrecision()\n",
    "        ])\n",
    "\n",
    "cnn = TornadoLikelihood()\n",
    "classifier = TornadoClassifier(cnn,metrics=metrics)\n",
    "\n",
    "# Low number of train_batches/epochs only for demo purposes\n",
    "trainer = L.Trainer(limit_train_batches=10, max_epochs=3)\n",
    "trainer.fit(classifier,train_dataloaders=torch_dl)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
